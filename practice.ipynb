{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ece95d9-c047-4fc5-9290-32c3c110479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ede63d-5e91-4c88-a994-047376770a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aff5dbb3-a71a-4989-a997-568932ba135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_sequence(X_data, Y_data, lookback):\n",
    "        \"\"\"\n",
    "        Creates input-output sequences from raw data arrays based on the lookback period.\n",
    "\n",
    "        :param X_data: Array of input data (features).\n",
    "        :param Y_data: Array of output data (targets).\n",
    "        :param lookback: The lookback window for creating sequences.\n",
    "        :return: Sequences of inputs and outputs.\n",
    "        \"\"\"\n",
    "        X_sequences, y_sequences = [], []\n",
    "        for i in range(lookback, len(X_data)):\n",
    "            X_sequences.append(X_data[i - lookback:i])\n",
    "            y_sequences.append(Y_data[i - lookback:i])\n",
    "\n",
    "        return np.array(X_sequences), np.array(y_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8579dd26-aec1-4f6c-a5b9-1efd6b2f6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data for training and testing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_df = pd.read_csv(\"Combined_Training31-Aug-2023.csv\")\n",
    "test_df = pd.read_csv(\"Combined_Testing31-Aug-2023.csv\")\n",
    "X_data_train = train_df[['SOC', 'Current', 'Temp']].values\n",
    "y_data_train =  train_df['Voltage'].values\n",
    "X_sequences_train, y_sequences_train = create_data_sequence(X_data_train, y_data_train, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "403115bd-d9da-4f6b-ab85-a9797ddcae27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((761802, 400, 3), (761802, 400))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sequences_train.shape, y_sequences_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "486f3fbb-ea38-4465-8543-97eca0b13368",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X_sequences_train, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_sequences_train, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b01880-acf4-4379-822e-c65919e54e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_tensor, 'X_tensor.pt')\n",
    "torch.save(y_tensor, 'y_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77b68b4d-97bb-4751-8756-43a7434cdc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler\n",
    "\n",
    "# Define parameters\n",
    "batch_size = 64\n",
    "train_split = 0.7\n",
    "seed = 42  # for reproducibility\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "\n",
    "# Calculate train and validation sizes\n",
    "train_size = int(dataset_size * train_split)\n",
    "valid_size = dataset_size - train_size\n",
    "\n",
    "# Shuffle the indices\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split indices into training and validation sets\n",
    "train_indices, valid_indices = indices[:train_size], indices[train_size:]\n",
    "\n",
    "# Create samplers for training and validation\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "\n",
    "# Create DataLoaders with samplers\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, drop_last=True)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80d6b7-1182-44f4-999a-5856ca77a784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "019d4007-7234-465e-99d7-9fdc996033ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size=3, output_size=1, embed_size=32, hidden_size=64, e_num_layers=1, d_num_layers=1, num_heads=4, dropout_prob=0.1, device=\"cpu\"):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Embedding layer for combined input (SOC, current, temperature)\n",
    "        self.embedding = nn.Linear(input_size, embed_size).to(self.device)\n",
    "\n",
    "        # Embedding layer for decoder input (voltage)\n",
    "        self.dec_embedding = nn.Linear(output_size, embed_size).to(self.device)\n",
    "\n",
    "        # Transformer encoder for input sequence\n",
    "        self.encoder = TransformerEncoder(\n",
    "            TransformerEncoderLayer(\n",
    "                d_model=embed_size,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_size,\n",
    "                dropout=dropout_prob,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=e_num_layers\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Transformer decoder for autoregressive prediction\n",
    "        self.decoder = TransformerDecoder(\n",
    "            TransformerDecoderLayer(\n",
    "                d_model=embed_size,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=hidden_size,\n",
    "                dropout=dropout_prob,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=d_num_layers\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Output layer to predict voltage at each timestep\n",
    "        self.output_layer = nn.Linear(embed_size, output_size).to(self.device)\n",
    "\n",
    "    def forward(self, X, dec_input):\n",
    "        \"\"\"\n",
    "        X: Input sequence containing (SOC, Current, Temperature), shape: (batch_size, seq_len, input_size)\n",
    "        dec_input: Voltage sequence for the decoder, shape: (batch_size, seq_len, output_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check input shapes\n",
    "        print(\"Shape of X before embedding:\", X.shape)  # Expected: (batch_size, seq_len, input_size)\n",
    "        print(\"Shape of dec_input before embedding:\", dec_input.shape)  # Expected: (batch_size, seq_len, output_size)\n",
    "        \n",
    "        # Embedding the combined input\n",
    "        X = self.embedding(X.to(self.device))  # Shape should be (batch_size, seq_len, embed_size)\n",
    "        print(\"Shape of X after embedding:\", X.shape)  # Expected: (batch_size, seq_len, embed_size)\n",
    "\n",
    "        # Encoder pass\n",
    "        encoder_output = self.encoder(X)  # Shape should be (batch_size, seq_len, embed_size)\n",
    "        print(\"Shape of encoder output:\", encoder_output.shape)  # Expected: (batch_size, seq_len, embed_size)\n",
    "\n",
    "        # Embedding the decoder input\n",
    "        dec_input = dec_input.unsqueeze(-1)\n",
    "        dec_input = self.dec_embedding(dec_input.to(self.device))  # Shape should be (batch_size, seq_len, embed_size)\n",
    "        print(\"Shape of dec_input after embedding:\", dec_input.shape)  # Expected: (batch_size, seq_len, embed_size)\n",
    "\n",
    "        # Create a mask for the decoder\n",
    "        tgt_mask = torch.triu(torch.ones(dec_input.size(1), dec_input.size(1)), diagonal=1).bool().to(self.device)\n",
    "        \n",
    "        # Decoder pass\n",
    "        decoder_output = self.decoder(dec_input, encoder_output, tgt_mask=tgt_mask)\n",
    "        print(\"Shape of decoder output:\", decoder_output.shape)  # Expected: (batch_size, seq_len, embed_size)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.output_layer(decoder_output)  # Final shape should be (batch_size, seq_len, output_size)\n",
    "        print(\"Shape of output after output layer:\", output.shape)  # Expected: (batch_size, seq_len, output_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def training_step(self, X, y_input, y_target, optimizer):\n",
    "        '''Training step for the transformer model'''\n",
    "    \n",
    "        # Move data to device\n",
    "        X, y_input, y_target = X.to(self.device), y_input.to(self.device), y_target.to(self.device)\n",
    "    \n",
    "        # Forward pass\n",
    "        output = self(X, y_input)  # X is the combined input; y_input is the decoder input\n",
    "    \n",
    "        # Compute loss between model output and target\n",
    "        loss = F.mse_loss(output, y_target)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        return loss.item()\n",
    "    \n",
    "    def validation_step(self, X, y_input, y_target):\n",
    "        '''Validation step for the transformer model'''\n",
    "    \n",
    "        # Move data to device\n",
    "        X, y_input, y_target = X.to(self.device), y_input.to(self.device), y_target.to(self.device)\n",
    "    \n",
    "        # Forward pass without gradient computation\n",
    "        with torch.no_grad():\n",
    "            output = self(X, y_input)  # X is the combined input; y_input is the decoder input\n",
    "        \n",
    "        # Compute validation loss\n",
    "        loss = F.mse_loss(output, y_target)\n",
    "    \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aefebb78-4618-4233-8db1-e6fc62f08ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(input_size=3, embed_size=32, hidden_size=64, e_num_layers=1, d_num_layers=1, num_heads=4, dropout_prob=0.1, device=\"cpu\"):\n",
    "    return TransformerModel(\n",
    "        input_size=input_size,\n",
    "        embed_size=embed_size,\n",
    "        hidden_size=hidden_size,\n",
    "        e_num_layers=e_num_layers,\n",
    "        d_num_layers=d_num_layers,\n",
    "        num_heads=num_heads,\n",
    "        dropout_prob=dropout_prob,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecfbb605-3401-40dc-af0b-042c9db182c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73412bbc-8e88-4375-8f33-456159750994",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = build_transformer(input_size=3, embed_size=16, hidden_size=64, e_num_layers=2, d_num_layers=2, num_heads=4, dropout_prob=0.1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a31e13e-fe93-4b09-977f-cea9182c3130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 15,473 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# caluclate the number of parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa6356d3-9580-43eb-a9e1-afdb87e66710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f0bdb71-1d19-48a3-942d-f426d2c73535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_batch shape: torch.Size([64, 400, 3])\n",
      "y_batch shape: torch.Size([64, 400])\n"
     ]
    }
   ],
   "source": [
    "# Fetch a single batch from train_loader\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "\n",
    "# Check the shape directly from the DataLoader\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "print(\"X_batch shape:\", X_batch.shape)  # Expected: (64, 400, 3)\n",
    "print(\"y_batch shape:\", y_batch.shape)  # Expected: (64, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d0da398-672a-4fda-876d-fa19935131b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X before embedding: torch.Size([64, 400, 3])\n",
      "Shape of dec_input before embedding: torch.Size([64, 400])\n",
      "Shape of X after embedding: torch.Size([64, 400, 16])\n",
      "Shape of encoder output: torch.Size([64, 400, 16])\n",
      "Shape of dec_input after embedding: torch.Size([64, 400, 16])\n",
      "Shape of decoder output: torch.Size([64, 400, 16])\n",
      "Shape of output after output layer: torch.Size([64, 400, 1])\n",
      "Output shape: torch.Size([64, 400, 1])\n",
      "Target shape: torch.Size([64, 400])\n"
     ]
    }
   ],
   "source": [
    "# Forward pass through the model\n",
    "output = model(X_batch, y_batch)\n",
    "\n",
    "# Check output shapes\n",
    "print(\"Output shape:\", output.shape)  # Expected shape: (batch_size, seq_len, 1)\n",
    "print(\"Target shape:\", y_batch.shape)  # Expected shape: (batch_size, seq_len, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1de9595-7b5b-4a14-988a-7e7d449b81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to be optimized\n",
    "    embed_size = trial.suggest_int(\"embed_size\", 16, 64)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 64, 256)\n",
    "    e_num_layers = trial.suggest_int(\"e_num_layers\", 1, 4)\n",
    "    d_num_layers = trial.suggest_int(\"d_num_layers\", 1, 4)\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 2, 8)\n",
    "    dropout_prob = trial.suggest_float(\"dropout_prob\", 0.1, 0.3)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1e-3)\n",
    "\n",
    "    # Initialize the Transformer model with suggested hyperparameters\n",
    "    model = TransformerModel(\n",
    "        input_size=3,\n",
    "        output_size=1,\n",
    "        embed_size=embed_size,\n",
    "        hidden_size=hidden_size,\n",
    "        e_num_layers=e_num_layers,\n",
    "        d_num_layers=d_num_layers,\n",
    "        num_heads=num_heads,\n",
    "        dropout_prob=dropout_prob,\n",
    "        device=device\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop (for a few epochs to get an idea of performance)\n",
    "    num_epochs = 5  # You can increase this for more thorough tuning\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            loss = model.training_step(X_batch, y_batch, optimizer)\n",
    "            train_loss += loss * X_batch.size(0)\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                loss = model.validation_step(X_val, y_val)\n",
    "                val_loss += loss * X_val.size(0)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        # Report validation loss to Optuna\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "\n",
    "        # Prune trial if it performs poorly\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return avg_val_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
